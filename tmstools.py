from imports import *
from utils import *
from stats import *

class TMSmap:
# class of a TMS map, containing the locations of stimuli and MEP amplitudes
    def __init__(self, loc, ampl, nSide, iX, iY):
    # constructor accepting arrays of stimulus locations and amplitudes arranged sequentially
    # it is assumed that the stimulation was performed using a square grid with nSide points along a side
    # the iX, iY variables define two coordinate axes used in the numbering of grid cells in the clusterPoints() method
        self.nSide = nSide; self.iX=iX; self.iY=iY
        self.loc = loc
        self.ampl1d = ampl
        self.clusterPoints()
    def clusterPoints(self):
    # arrange the stimulus data into a 2d array corresponding to the stimulation grid
    # the arrangement is based on clustering the stimulus locations into nSide**2 groups
        kmeans = KMeans(n_clusters=self.nSide*self.nSide, random_state=0).fit(self.loc)
        self.labels = kmeans.labels_
        c=Counter(kmeans.labels_)
        self.centers = np.array([np.mean(self.loc[kmeans.labels_ == i],axis=0) for i in range(self.nSide**2)])
        minXminY = np.argmin([2*x[self.iX]+x[self.iY] for x in self.centers])
        iBasisPair = np.array([norm(v-self.centers[minXminY]) for v in self.centers]).argsort()[1:3]
        self.ls = np.zeros((self.nSide,self.nSide),dtype = np.int)
        # arrange the cluster labels into a 2d array corresponding to the stimulation grid:
        self.ls[0,0] = minXminY
        if self.centers[iBasisPair[0]][self.iX]-self.centers[iBasisPair[0]][self.iY] > self.centers[iBasisPair[1]][self.iX]-self.centers[iBasisPair[1]][self.iY]:
            self.ls[1,0] = iBasisPair[0]; self.ls[0,1] = iBasisPair[1]
        else:
            self.ls[1,0] = iBasisPair[1]; self.ls[0,1] = iBasisPair[0]
        for i in range(2,self.nSide):
            approxVec = 2*self.centers[self.ls[i-1,0]] - self.centers[self.ls[i-2,0]]
            self.ls[i,0] = np.argmin([norm(v-approxVec) for v in self.centers])
        for j in range(2,self.nSide):
            approxVec = 2*self.centers[self.ls[0,j-1]] - self.centers[self.ls[0,j-2]]
            self.ls[0,j] = np.argmin([norm(v-approxVec) for v in self.centers])
        for i in range(1,self.nSide):
            for j in range(1,self.nSide):
                approxVec = self.centers[self.ls[i,j-1]] + self.centers[self.ls[i-1,j]] - self.centers[self.ls[i-1,j-1]]
                self.ls[i,j] = np.argmin([norm(v-approxVec) for v in self.centers])
        self.cs = np.array([[self.centers[self.ls[i,j]] for j in range(self.nSide)] for i in range(self.nSide)])
        self.ampl = np.array([[self.ampl1d[self.labels==self.ls[i,j]] for j in range(self.nSide)] for i in range(self.nSide)])
        self.nStimArr = np.array([[len(x) for x in line] for line in self.ampl])
        self.cellLen = 0.5 * (norm(self.cs[self.nSide-1,self.nSide-1]-self.cs[0,0])
                            + norm(self.cs[self.nSide-1,0]-self.cs[0,self.nSide-1])) /np.sqrt(2) / (self.nSide-1)
        self.cellArea = self.cellLen**2
        
    def pPositive(self, thr = 50):
    # probabilities (estimated by frequencies) of positive (suprathreshold) MEPs in all grid cells
        return np.array([[sum(ampls > thr) / len(ampls) for ampls in [self.ampl[i,j] for j in range(self.nSide)]] for i in range(self.nSide)])

def randMapK(M, k, ind1 = None, ind2 = None):
# generate a map with k stimuli per cell by bootstrapping from a given map M
# the MEPs in each cell are generated by sampling with replacement from the MEPs in the same cell of M
    return np.array([[np.random.choice(x[ind1:ind2], k) for x in line] for line in M.ampl])

# Representation parameters
# The area-like parameters are calculated in the units of the grid cell area
# The COG coordinates are calculated in the units of the grid cell length
    
dmt = 50 # default MEP threshold, 50 uV
def maxArr(MEParray, takeFirst = None, thr = dmt):  # MEParray: nSide x nSide x nStim (num stim per cell, may be variable)
# array of the maximum MEP amplitudes in all grid cells
# 'takeFirst' can be used to restrict the calculation to the first several stimuli in each grid cell
# 'thr' is the MEP amplitude threshold
    return np.array([[max(chopSmall(ampls[0:takeFirst], thr=thr)) for ampls in line] for line in MEParray])

def maxMEPthrArea(MEParray, takeFirst = None, thr = dmt):
# area of the cells with maximum MEP amplitudes above a threshold
    return np.sum(maxArr(MEParray, takeFirst, thr) > thr)

def meanArr(MEParray, takeFirst = None, thr = dmt):
# array of the mean MEP amplitudes in all grid cells
    return np.array([[np.mean(chopSmall(ampls[0:takeFirst], thr=thr)) for ampls in line] for line in MEParray])   

def meanMEPthrArea(MEParray, takeFirst = None, thr = dmt):
# area of the cells with mean MEP amplitudes above a threshold
    return np.sum(meanArr(MEParray, takeFirst, thr) > thr)

def probArr(MEParray, takeFirst = None, thr = dmt):
# array of the probabilities of suprathreshold MEPs in all grid cells
    return np.array([[sum(ampls[0:takeFirst] > thr) / len(ampls[0:takeFirst]) for ampls in line] for line in MEParray])

def probWarea(MEParray, takeFirst = None, thr = dmt):
# area weighted by the probability of a suprathreshold MEP
    return np.sum(probArr(MEParray, takeFirst, thr))

def meanWarea(MEParray, takeFirst = None, thr = dmt):
# area weighted by the mean MEP amplitude
    return np.sum(meanArr(MEParray, takeFirst, thr))

def probHalfThrArea(MEParray, takeFirst = None, thr = dmt):
# area of the cells with the probability of a suprathreshold MEP above 0.5
    return np.sum(probArr(MEParray, takeFirst, thr) > 0.5)
    
def COGbyMax(MEParray, takeFirst = None, thr = dmt):
# center of gravity with the weights defined as the maximal amplitudes in each grid cell
    nSide = len(MEParray)
    maxArr1 = maxArr(MEParray, takeFirst, thr)
    s = np.sum(maxArr1)
    if s == 0:
        return np.array([np.nan,np.nan])
    else:
        return np.array([np.sum([i*maxArr1[i,j] for i in range(nSide) for j in range(nSide)]) / s,
                         np.sum([j*maxArr1[i,j] for i in range(nSide) for j in range(nSide)]) / s])

def COGbyMean(MEParray, takeFirst = None, thr = dmt):
# center of gravity with the weights defined as the mean amplitudes in each grid cell
    nSide = len(MEParray)
    meanArr1 = meanArr(MEParray, takeFirst, thr)
    s = np.sum(meanArr1)
    if s == 0:
        return np.array([np.nan,np.nan])
    else:
        return np.array([np.sum([i*meanArr1[i,j] for i in range(nSide) for j in range(nSide)]) / s,
                         np.sum([j*meanArr1[i,j] for i in range(nSide) for j in range(nSide)]) / s])

def COGbyProb(MEParray, takeFirst = None, thr = dmt):
# center of gravity with the weights defined as the probabilities of suprathreshold MEPs in each grid cell
    nSide = len(MEParray)
    probArr1 = probArr(MEParray, takeFirst, thr)
    s = np.sum(probArr1)
    if s == 0:
        return np.array([np.nan,np.nan])
    else:
        return np.array([np.sum([i*probArr1[i,j] for i in range(nSide) for j in range(nSide)]) / s,
                         np.sum([j*probArr1[i,j] for i in range(nSide) for j in range(nSide)]) / s])

# Statistical characteristics of parameters

def paramRelBiasArr(paramFunc, boots, maps, takeFirstInTrue=None, thr = dmt):  # format: paramFunc(MEParray, takeFirst = None, thr = 50)
# compute an array of the normalized bias values for a TMS map parameter computed by the function paramFunc
# the reference ('true') values are computed from the initial mapping data, 'maps': subject x session nested list of TMSmap objects
# the bias values are computed for an array of maps generated by bootstrapping and passed in the argument 'boots'
# the dimensions of 'boots' are: subject, bootstrapping sample, session, grid index 1, grid index 2, stimulus index
    nSubj = boots.shape[0]; nSess = boots.shape[2]; nStimMax = boots.shape[-1];
    biasArr = np.zeros((nStimMax+1, nSubj, nSess))
    for iSubj in range(nSubj):
        for iSess in range(nSess):
            trueVal = paramFunc(maps[iSubj][iSess].ampl, takeFirst=takeFirstInTrue, thr = thr)
            for nStim in range(1,nStimMax+1):
                biasArr[nStim][iSubj][iSess] = np.mean([paramFunc(MEParray, takeFirst=nStim, thr = thr) for MEParray in boots[iSubj,:,iSess]]) / trueVal - 1
    return biasArr

def paramCVarr(paramFunc, boots, thr = dmt):
# compute an array of the values of the coefficient of variation (within a TMS mapping session)
# for a TMS map parameter computed by the function paramFunc
    nSubj = boots.shape[0]; nSess = boots.shape[2]; nStimMax = boots.shape[-1];
    CVarr = np.zeros((nStimMax+1, nSubj, 3))
    for nStim in range(1,nStimMax+1):
         for iSubj in range(nSubj):
            for iSess in range(nSess):
                CVarr[nStim][iSubj][iSess] = CV([paramFunc(MEParray, takeFirst=nStim, thr = thr) for MEParray in boots[iSubj,:,iSess]])
    return CVarr

def paramRelRMSDarr(paramFunc, boots, maps, takeFirstInTrue=None, thr = dmt):
# compute an array of the values of the normalized RMSD (within a TMS mapping session)
# for a TMS map parameter computed by the function paramFunc
    nSubj = boots.shape[0]; nSess = boots.shape[2]; nStimMax = boots.shape[-1];
    RMSDarr = np.zeros((nStimMax+1, nSubj, nSess))
    for iSubj in range(nSubj):
        for iSess in range(nSess):
            trueVal = paramFunc(maps[iSubj][iSess].ampl, takeFirst=takeFirstInTrue, thr = thr)
            for nStim in range(1,nStimMax+1):
                RMSDarr[nStim][iSubj][iSess] = np.sqrt(np.mean([(paramFunc(MEParray, takeFirst=nStim, thr = thr) / trueVal - 1)**2 for MEParray in boots[iSubj,:,iSess]]))
    return RMSDarr

def halfRelDiff(v):  # half the relative difference of max and min, used as a measure of variability between sessions
    return (max(v) - min(v)) / (max(v) + min(v))

def paramHalfRelDiffArr(paramFunc, boots, thr = dmt):
# compute an array of the values of the between-session variability index
# (defined as half the relative difference between max and min among the sessions)
# of a TMS map parameter computed by the function paramFunc
    nSubj = boots.shape[0]; nSess = boots.shape[2]; nStimMax = boots.shape[-1];
    paramVar = np.zeros((nStimMax+1, nSubj))
    for nStim in range(1,nStimMax+1):
        for iSubj in range(nSubj):
            hrd = np.array([halfRelDiff([paramFunc(MEParray, takeFirst=nStim, thr=thr) for MEParray in MEParraysForAllSessions])
                                                  for MEParraysForAllSessions in boots[iSubj]])
            paramVar[nStim][iSubj] = np.mean(hrd[~np.isnan(hrd)])
    return paramVar

def paramArrForNstim(paramFunc, boots, takeFirst = None, thr = dmt):
# compute an array of the values of a TMS map parameter computed by the function paramFunc
# for an array of bootstrapping-generated maps
    return np.array([
        np.transpose([[paramFunc(MEParray, takeFirst=takeFirst, thr = thr) for MEParray in MEParraysForAllSessions]
            for MEParraysForAllSessions in subjBoots]) for subjBoots in boots])

def vecParamMeanDistFromTrue(paramFunc, boots, maps, takeFirstInTrue=None, thr = dmt):
# compute an array of the accuracy values of a vector parameter computed by the function paramFunc
# the accuracy is defined as the mean distance between the vectors from bootstrapping-generated maps and
# the vectors from the initial maps
    nSubj = boots.shape[0]; nSess = boots.shape[2]; nStimMax = boots.shape[-1];
    meanDistArr = np.zeros((nStimMax+1, nSubj, 3))
    for iSubj in range(nSubj):
        for iSess in range(nSess):
            trueVec = paramFunc(maps[iSubj][iSess].ampl, takeFirst=takeFirstInTrue, thr = thr)
            for nStim in range(1,nStimMax+1):
                meanDistArr[nStim][iSubj][iSess] = meanWithoutNAN([
                    np.linalg.norm(paramFunc(MEParray, takeFirst=nStim, thr = thr) - trueVec)
                    for MEParray in boots[iSubj,:,iSess]])
    return meanDistArr

# Plotting
    
def plotParamHists(arr, paramFunc, maps, takeFirstInTrue = None, thr = dmt, xlab = 'Parameter', unit = 1):
# plot histograms of the values corresponding to bootstrapping-generated maps
# for a parameter computed by the function paramFunc, for all sessions of all subjects
    arr = unit * arr
    nSubj = arr.shape[0]; nSess = arr.shape[1]
    f,ax = plt.subplots(int(nSubj/2),2)
    cols = ['r','g','b']
    for iSubj in range(nSubj):
        plt.sca(ax[int(iSubj/2),iSubj%2])
        for iSess in range(nSess):
            plt.hist(arr[iSubj,iSess],alpha = 0.3, color = cols[iSess])
            plt.xlim(0,1.1 * np.max(arr[iSubj]))
            paramFromFullMap = unit * paramFunc(maps[iSubj][iSess].ampl, takeFirst = takeFirstInTrue, thr = thr)
            plt.plot([paramFromFullMap,paramFromFullMap], [0,300 - iSess * 25], color = cols[iSess])
            plt.text(0.06 * np.max(arr[iSubj]),250,'Subject %d'%(iSubj+1))
        plt.title('BICC: %2.2f, overlaps: 1-2: %2.2f, 2-3: %2.2f, 1-3: %2.2f'%(ICConeWay(arr[iSubj]),
          overlap(arr[iSubj,0], arr[iSubj,1]), overlap(arr[iSubj,1], arr[iSubj,2]), overlap(arr[iSubj,0], arr[iSubj,2])))
        if iSubj == 0:
            plt.legend(['Session 1','Session 2','Session 3'], loc = 'lower left')
            plt.xlabel(xlab)
            plt.ylabel('Frequency')
    setSize(10,8)
    plt.tight_layout()

def medianPlot(arr, col = None, linestyle = None, label = None, isEvenNstim = False, isOddNstim = False):  # arr: lastNstim+1 x nSubj
# plot medians of all lines in an array 'arr'
# the first dimension of 'arr' corresponds to the number of stimuli per cell and can be restricted to even values
    if isEvenNstim:
        nStimToPlot = np.arange(2, len(arr), 2)
    elif isOddNstim:
        nStimToPlot = np.arange(1, len(arr), 2)
    else:
        nStimToPlot = np.arange(1,len(arr))
    plt.plot(nStimToPlot, [np.median(arr[nStim]) for nStim in nStimToPlot],
             'o-',color = col, linestyle = linestyle, label = label)
    plt.xlabel('Number of stimuli per grid cell')

def allMapsMedianPlot(arr, col = None, linestyle = None, label = None, isEvenNstim = False, isOddNstim = False):
# arr: lastNstim+1 x nSubj x nSess
# plot medians of all 2d arrays defined by slices of 'arr' along the first axis (number of stimuli per cell)
    arrFlattenedMaps = np.array([flatten2d(maps) for maps in arr])
    medianPlot(arrFlattenedMaps, col = col, linestyle = linestyle, label = label, isEvenNstim = isEvenNstim, isOddNstim = isOddNstim)
    
# Testing

def PageAndFriedmanTests(arr, isEvenNstim = False, isOddNstim = False): # arr: lastNstim+1 x nSubj x nSess
# Page's nonparametric test for ordered alternatives in a randomized complete block design
# and Friedman's nonparametric test for general alternatives in the same design
# we use the implementation in the R language ('crank' package) through the rpy2 interface between R and Python
    nSubj = arr.shape[1]
    if isEvenNstim:
        nStimToTest = np.arange(2, len(arr), 2)
    elif isOddNstim:
        nStimToTest = np.arange(1, len(arr), 2)
    else:
        nStimToTest = np.arange(1,len(arr))
    arrToTest = arr[nStimToTest]
    rV = robjects.FloatVector(flatten2d(np.transpose(arrToTest[::-1])))
    # Page's test is against increasing values. We test against decreasing
    m = robjects.r.matrix(rV, nrow = nSubj, byrow=True)
    res = crank.page_trend_test(m, ranks = False)
    return 'Page: ' + str(res.rx(4)).split()[-1] + ', Friedman: %2.1e'%scipy.stats.friedmanchisquare(*arrToTest)[1]
    
def PageAndFriedmanTestsForMeansBySessions(arr, isEvenNstim = False, isOddNstim = False): # arr: lastNstim+1 x nSubj x nSess
# the Page's and Friedman's tests applied to the mean values (by sessions) for every subject
    meansBySessions = np.mean(arr, axis = 2)
    return PageAndFriedmanTests(meansBySessions, isEvenNstim = isEvenNstim, isOddNstim = isOddNstim)
   